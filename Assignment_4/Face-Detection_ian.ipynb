{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pictures that I used were of the band the xx. The reasoning behind this was that photographs of the band and its individual members were widely available on the internet. These images show a diverse set of backgrounds and facial expressions which allow for data training to be done successfully. In addition to finding the images, I attempted to crop them so that the faces were oriented the same way in the frame with the eyes and mouth being situated in the same region so that it would be easy to perform facial recognition on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNearestNeighborNormalImages(images):\n",
    "    #the total number of correct we find in the end\n",
    "    totalCorrect = 0\n",
    "    #peform for each of the images\n",
    "    for image in images:\n",
    "        #set the nearest distance to something outrageous and have the nearest image be empty\n",
    "        nearestDistance = 100000000\n",
    "        nearestFace = \"\"\n",
    "        \n",
    "        #for each of the images that is not the current image itself, peform a euclidean distance calculation\n",
    "        #store the closest neighbor \n",
    "        for currentImage in images:\n",
    "            if (image == currentImage):\n",
    "                continue\n",
    "            else:\n",
    "                dist = np.abs(np.linalg.norm(image[0]-currentImage[0]))\n",
    "                if (dist < nearestDistance):\n",
    "                    nearestDistance = dist\n",
    "                    nearestFace = currentImage[1]\n",
    "        #if the name we recieve as the closest neighbor is the same as the image we are testing on we are correct\n",
    "        if(str(image[1]) == nearestFace):\n",
    "            totalCorrect = totalCorrect+1\n",
    "    print(totalCorrect, \"out of 30 are correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNearestNeighborEigenvectors(vector,eigenVectors):\n",
    "\n",
    "    #set the nearest distance to something outrageous and have the nearest vector be empty\n",
    "    nearestDistance = 100000000\n",
    "    nearestFace = \"\"\n",
    "\n",
    "    #for each of the vectors that is not the current vector itself, peform a euclidean distance calculation\n",
    "    #store the closest neighbor \n",
    "    for currectVector in eigenVectors:\n",
    "        if (vector == currectVector):\n",
    "            continue\n",
    "        else:\n",
    "            dist = distance.euclidean(vector[0], currectVector[0])\n",
    "            if (dist < nearestDistance):\n",
    "                nearestDistance = dist\n",
    "                nearestFace = currectVector[1]\n",
    "    #if the name we recieve as the closest neighbor is the same as the vector we are testing on we are correct\n",
    "    if(str(vector[1]) == nearestFace):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to create a sliding window which can run through the image to test whether there is a face in each position\n",
    "def slidingWindow(image, sizeOfStep, sizeOfWindow):\n",
    "    #for each posible window position within the given range, yeild just the part of the image inside the window\n",
    "    for y in range(0, image.shape[0], sizeOfStep):\n",
    "        for x in range(0, image.shape[1], sizeOfStep):\n",
    "            yield (x, y, image[y:y + sizeOfWindow[1], x:x + sizeOfWindow[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectOntoEigenspace(image, mean, eigenvectors):\n",
    "    # Project our image into the eigenspace (eigenvectors)\n",
    "    image = np.array(image).flatten()\n",
    "    \n",
    "    # Normalize by substracting the average face\n",
    "    image = image - mean\n",
    "    image = np.reshape(image, (mean.shape[0], 1))\n",
    "    \n",
    "    # Projecting the image into the eigenspace\n",
    "    projectedImage = eigenvectors.transpose() * image\n",
    "    \n",
    "    return projectedImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_eigenspace(window, mean, weights, eigenvectors):\n",
    "    \n",
    "    projectedImage = projectOntoEigenspace(window, mean, eigenvectors)\n",
    "    \n",
    "    # return the twice normalized value of the weights minus the window on the eigenspace\n",
    "    norms = np.linalg.norm(np.linalg.norm(weights - projectedImage, axis=0), axis=0)\n",
    "    \n",
    "    return norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropWindow(image, startY, startX, sizeY, sizeX):\n",
    "    y,x = image.shape\n",
    "    return image[startY:startY+sizeY,startX:startX+sizeX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyImage(image,mean, eigenvectors, weights):\n",
    "    \n",
    "    #find the image projected onto the eigenspace\n",
    "    projectedImage = projectOntoEigenspace(image, mean, eigenvectors) \n",
    "    \n",
    "    #find the difference between the importance of the eigenvector and the newly found projected image\n",
    "    diff = weights - projectedImage\n",
    "    \n",
    "    #normalize the difference\n",
    "    norms = np.linalg.norm(diff, axis=0)\n",
    "    \n",
    "    closestFaces = []\n",
    "    \n",
    "    #find the value representing the closest face\n",
    "    for i in range(0, 20):\n",
    "        closestFace = np.argmin(norms)\n",
    "        closestFaces.append(closestFace)\n",
    "        norms[closestFace] = 100000000\n",
    "    \n",
    "    print(closestFaces)\n",
    "    return(closestFaces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainData():\n",
    "    #obtain the file list for each of the sets of images\n",
    "    #create a numpy array of each of the sets of images then create one which has the images flattened\n",
    "    filelist = glob.glob('training_data/images1/*.*')\n",
    "\n",
    "    x = np.array([np.array(cv2.imread(fname,0), dtype='float64') for fname in filelist])\n",
    "\n",
    "    numImages1 = x.shape[0]\n",
    "    sizeImages1 = x.shape[1]*x.shape[2]\n",
    "\n",
    "    trainData1 = x.reshape([numImages1,sizeImages1])\n",
    "\n",
    "\n",
    "    filelist2 = glob.glob('training_data/images2/*.*')\n",
    "    y = np.array([np.array(cv2.imread(fname,0), dtype='float64') for fname in filelist2])\n",
    "\n",
    "    numImages2 = y.shape[0]\n",
    "    sizeImages2 = y.shape[1]*y.shape[2]\n",
    "\n",
    "    trainData2 = y.reshape([numImages2,sizeImages2])\n",
    "\n",
    "\n",
    "    filelist3 = glob.glob('training_data/images3/*.*')\n",
    "    z = np.array([np.array(cv2.imread(fname,0), dtype='float64') for fname in filelist3])\n",
    "\n",
    "    numImages3 = z.shape[0]\n",
    "    sizeImages3 = z.shape[1]*z.shape[2]\n",
    "\n",
    "    trainData3 = z.reshape([numImages3,sizeImages3])\n",
    "\n",
    "    #create an array with all of the flattened images together in one\n",
    "    totalData = np.swapaxes(np.concatenate((trainData1, trainData2, trainData3)),0,1)\n",
    "    return(totalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7b0af2074110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnumImages1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msizeImages1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrainData1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumImages1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msizeImages1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "#obtain the file list for each of the sets of images\n",
    "#create a numpy array of each of the sets of images then create one which has the images flattened\n",
    "filelist = glob.glob('training_data/images1/*.*')\n",
    "\n",
    "x = np.array([np.array(cv2.imread(fname,0), dtype='float64') for fname in filelist])\n",
    "\n",
    "numImages1 = x.shape[0]\n",
    "print(x.shape)\n",
    "sizeImages1 = x.shape[1]*x.shape[2]\n",
    "\n",
    "trainData1 = x.reshape([numImages1,sizeImages1])\n",
    "\n",
    "\n",
    "filelist2 = glob.glob('training_data/images2/*.*')\n",
    "y = np.array([np.array(cv2.imread(fname,0), dtype='float64') for fname in filelist2])\n",
    "\n",
    "numImages2 = y.shape[0]\n",
    "sizeImages2 = y.shape[1]*y.shape[2]\n",
    "\n",
    "trainData2 = y.reshape([numImages2,sizeImages2])\n",
    "\n",
    "\n",
    "filelist3 = glob.glob('training_data/images3/*.*')\n",
    "z = np.array([np.array(cv2.imread(fname,0), dtype='float64') for fname in filelist3])\n",
    "\n",
    "numImages3 = z.shape[0]\n",
    "sizeImages3 = z.shape[1]*z.shape[2]\n",
    "\n",
    "trainData3 = z.reshape([numImages3,sizeImages3])\n",
    "\n",
    "#create an array with all of the flattened images together in one\n",
    "totalData = np.swapaxes(np.concatenate((trainData1, trainData2, trainData3)),0,1)\n",
    "\n",
    "\n",
    "#find the pca representation all all three sets of training data and then transform the images using pca.transform\n",
    "pca1 = PCA(n_components=10, svd_solver='randomized',whiten=True).fit(trainData1)\n",
    "pca2 = PCA(n_components=10, svd_solver='randomized',whiten=True).fit(trainData2)\n",
    "pca3 = PCA(n_components=10, svd_solver='randomized',whiten=True).fit(trainData2)\n",
    "\n",
    "\n",
    "\n",
    "#plot each of the training data's explained variance ratios to show how many components we will need for accurate eigenfaces\n",
    "plt.plot(np.cumsum(pca1.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.cumsum(pca2.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.cumsum(pca3.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, as can be seen by the plot above, you do not need all of the vectors to represent the data as the cumulative variance reached 1 much sooner than the total number of vectors of the data. In addition, it can be seen that a very large percentage of the information is found within the first couple of vectors, with each additional vector providing less and less information. This is shown in the graphs for all three of the sets of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of all of the images together and then add labels to each of them to say which set they belong to\n",
    "images = []\n",
    "\n",
    "for image in x:\n",
    "    images.append([image, \"Jamie\"])\n",
    "for image in y:\n",
    "    images.append([image, \"Oliver\"])\n",
    "for vector in z:\n",
    "    images.append([image, \"Romy\"])\n",
    "#compute the l2 distance of all the images and output the ratio of images whose nearest neighbor is the same person\n",
    "computeNearestNeighborNormalImages(np.array(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the means and eigenvectors off all of the images\n",
    "mean1, eigenvectors1 = cv2.PCACompute(trainData1, mean=None)\n",
    "mean2, eigenvectors2 = cv2.PCACompute(trainData2, mean=None)\n",
    "mean3, eigenvectors3 = cv2.PCACompute(trainData3, mean=None)\n",
    "\n",
    "#create a list of all of the eigenvectors together and then add labels to each of them to say which set they belong to\n",
    "eigenvectors = []\n",
    "for vector in eigenvectors1:\n",
    "    eigenvectors.append([vector, \"Jamie\"])\n",
    "for vector in eigenvectors2:\n",
    "    eigenvectors.append([vector, \"Oliver\"])\n",
    "for vector in eigenvectors3:\n",
    "    eigenvectors.append([vector, \"Romy\"])\n",
    "\n",
    "    \n",
    "#the total number of correct we find in the end\n",
    "totalCorrect = 0\n",
    "#convert eigenvectors to numpy array\n",
    "eigenvectorArray = np.array(eigenvectors)\n",
    "\n",
    "#compute the l2 distance of all the eigenvectors and output the ratio of eigenvectors whose nearest neighbor is the same person\n",
    "for vector in eigenvectorArray:\n",
    "    val = computeNearestNeighborEigenvectors(vector, eigenvectors)\n",
    "    if val == 1:\n",
    "        totalCorrect = totalCorrect+1\n",
    "\n",
    "\n",
    "print(totalCorrect, \"out of 30 are correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "One might be able to expect an improvement in the amount of images which are correctly mapped to their give set. This would be because the eigenvector representation will mitigate the effects that things like the backgroud of an image will have on the difference between the two images. For example, in the case where two images of different sets have the exact same background, in the normal image representation they are likely to have each other as nearest neighbors. However, in the eigenspace, these types of differences will impact the nearest neighbors calculation far less than in the normal space leading to a higher likelihood of correct nearest neighbors as is show above where in the original space only 20/30 were correct and in the eigenspace all 30 out of 30 were correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 Face Detector and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reobtain the data - I was having some issues with this so I'm just doing it again here\n",
    "totalData = obtainData()\n",
    "\n",
    "# Calculate the average face\n",
    "meanFace = np.sum(totalData, axis=1) \n",
    "meanFace = meanFace / 30\n",
    "\n",
    "# Normalize the dataset's images by substraction the average face\n",
    "for j in range(0, 30):\n",
    "    totalData[:, j] -= meanFace[:]\n",
    "\n",
    "# Find the normalized covariance matrix of the data \n",
    "dataTrans= np.matrix(totalData.transpose())\n",
    "cov_matrix = dataTrans * np.matrix(totalData)\n",
    "cov_matrix = cov_matrix / 30\n",
    "\n",
    "# Find the eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort the eigenvalues and eigenvectors in order of the size of the eigenvalue\n",
    "# eigenvectors should be sorted to correspond with eigenvalues\n",
    "sort_indices = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[sort_indices]\n",
    "eigenvectors = eigenvectors[:,sort_indices]\n",
    "\n",
    "#find the total value of all of the eigenvalues\n",
    "eigenvalues_sum = 0\n",
    "for i in range(0,len(eigenvalues)):\n",
    "    eigenvalues_sum = eigenvalues_sum + eigenvalues[i]\n",
    "\n",
    "\n",
    "enumEigenvalues = 0\n",
    "totalVariance = 0.0\n",
    "\n",
    "#find the number of eigenvalues needed to obtain 85% of the variance\n",
    "for i in range (0, len(eigenvalues)):\n",
    "    totalVariance = totalVariance + eigenvalues[i]/ eigenvalues_sum\n",
    "    if totalVariance >= 0.85:\n",
    "        enumEigenvalues = i+1\n",
    "        break\n",
    "\n",
    "#take only the eigenvectors which are pertinant\n",
    "importantEigenvectors = eigenvectors[:,0:enumEigenvalues]\n",
    "\n",
    "\n",
    "importantEigenvectors = totalData * importantEigenvectors\n",
    "norms = np.linalg.norm(importantEigenvectors, axis=0)\n",
    "importantEigenvectors = importantEigenvectors / norms\n",
    "\n",
    "eigenTranspose = importantEigenvectors.transpose()\n",
    "weights =  eigenTranspose * totalData\n",
    "\n",
    "image = np.array(cv2.imread(\"sherlock_cast.png\",0), dtype='float64')\n",
    "faceDetectImage = image.copy()\n",
    "imageList = []\n",
    "\n",
    "for (x, y, window) in slidingWindow(image, sizeOfStep=32, sizeOfWindow=(imageShape)):\n",
    "    # if the window does not meet our desired window size, ignore it\n",
    "    if window.shape[0] != winH or window.shape[1] != winW:\n",
    "        continue\n",
    "        \n",
    "    # find the similarity between the current window and our eigenspace for faces\n",
    "    faceLikelihood = distance_eigenspace(window, meanFace, weights, importantEigenvectors)\n",
    "    \n",
    "    #set the threshold so that only faces come through from the data\n",
    "    if (faceLikelihood < 56500):\n",
    "        cv2.rectangle(faceDetectImage, (x, y), (x + winW, y + winH), (0,0,255), 2)\n",
    "        window = cropWindow(image,y,x,winH,winW)\n",
    "        imageList.append(np.array(window.tolist()))\n",
    "\n",
    "for i in range(0,len(imageList)):\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.subplot(111)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.imshow(imageList[i], cmap=\"gray\")\n",
    "    imageClassification = classifyImage(imageList[i],meanFace,importantEigenvectors,weights)\n",
    "    if(imageClassification < 10):\n",
    "        print('image classified as jamie')\n",
    "    elif(imageClassification < 20):\n",
    "        print('image classified as oliver')\n",
    "    else:\n",
    "        print('image classified as romy')\n",
    "        \n",
    "plt.figure(figsize = (10,10))\n",
    "plt.title(\"Detected Faces\"), plt.xticks([]), plt.yticks([])\n",
    "plt.imshow(faceDetectImage, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method as a whole works somewhat well. By in large, it provides the correct person as the closest neighbour about 1/2 of the time (this scales as well when you set the threshold to a higher number and thus have more faces to work with) which is a higher rate than one would expect for random given three people. However, This is also not exactly a great face classification as the faces that are being provided are generally well centered and thus one might expect a higher success rate than 1/2. \n",
    "\n",
    "With regards to false positives for the facial detection parts, there are no \"false positives\" as all of the images under the threshold are in fact faces. However, none of the images are perfectly centered and there are two found for one of the three people in the photo. In addition, as will be again touched on later, the threshold is set based on the image itself and thus must be recalibrated for the specific image that is being worked on. For this reason, it is somewhat hard to guage \"false positives\" and more simple to say that the algoritm will generally detect faces in the frame but will not necesarily provide the best version of the faces and may provide each face more than once.\n",
    "\n",
    "There are two mis-identified faces when working with the current threshold. The first and fourth detected faces are in face correct. The second face will take four tries to find the correct nearest neighbor and the third will take 13 which is very far off. \n",
    "\n",
    "The largest problem that I see regarding this algorithm is that it does not account for the different sizes of peoples head's in the frame. If one were to more accurately cut out only the faces of the people in the frame it would likely remove much of the error as was seen from the nearest neighbor calculations done on the test data. One would expect this to cause a large bit of the error as the well centered faces seem to be well mapped using the nearest neighbor calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OpenCV datapath\n",
    "data_path = cv2.data.haarcascades\n",
    "face_cascade = cv2.CascadeClassifier( os.path.join(data_path, 'haarcascade_frontalface_default.xml'))\n",
    "eye_cascade = cv2.CascadeClassifier( os.path.join(data_path, 'haarcascade_eye.xml'))\n",
    "\n",
    "img = cv2.imread('TestImage.jpeg')\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "#uses Viola-Jones implementation to detect faces in the test image that was input as shown intutorial\n",
    "#should only run on faces rather than also executing for eyes\n",
    "faces = face_cascade.detectMultiScale(img_gray, 1.1,2)\n",
    "\n",
    "#should draw a rectangle around all of the faces that were found by the face detector\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "\n",
    "# display images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Input Image\"), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen based on the two images, the Viola-Jones method does a better job of finding the correct matches in the data than the one that has been created from scratch. While my detector does not have a lot of false positives, this is in large part because of the threshold which has been set that is somewhat arbitrary. Given a different image set, one would need to use a different threshold for the image in order to ensure that detection was performed correctly while with Viola-Jones this does not need to be done which allows it to be more successful in detecting faces than my algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
